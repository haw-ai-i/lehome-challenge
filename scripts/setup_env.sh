#!/usr/bin/env bash
set -euo pipefail
set -x

log() {
  printf '[setup_env] %s\n' "$*" >&2
}

clone_or_fetch_isaaclab() {
  local target_dir="$1"
  local archive_url="https://github.com/lehome-official/IsaacLab/archive/refs/heads/main.tar.gz"
  local parent_dir
  parent_dir="$(dirname "${target_dir}")"

  if [[ -d "${target_dir}/.git" || -f "${target_dir}/isaaclab.sh" ]]; then
    log "IsaacLab already present at ${target_dir}; skipping fetch"
    return 0
  fi

  rm -rf "${target_dir}"
  mkdir -p "${parent_dir}"

  if command -v uv >/dev/null 2>&1; then
    log "Downloading IsaacLab archive with uv-managed python"
    uv run --no-project python - "${archive_url}" "${parent_dir}" "${target_dir}" <<'PY'
import os
import shutil
import sys
import tarfile
import tempfile
import urllib.request

archive_url = sys.argv[1]
parent_dir = sys.argv[2]
target_dir = sys.argv[3]

with tempfile.NamedTemporaryFile(delete=False) as tmp:
    tmp_path = tmp.name

try:
    urllib.request.urlretrieve(archive_url, tmp_path)
    with tarfile.open(tmp_path, "r:gz") as tar:
        tar.extractall(parent_dir)
finally:
    if os.path.exists(tmp_path):
        os.remove(tmp_path)

extracted_dir = os.path.join(parent_dir, "IsaacLab-main")
if os.path.exists(target_dir):
    shutil.rmtree(target_dir)
shutil.move(extracted_dir, target_dir)
PY
    return 0
  fi

  log "Missing uv; cannot fetch IsaacLab"
  return 1
}

# Root of the project snapshot (contains repo/ and run_metadata/)
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

# Load variables from .env if present so downstream commands inherit them
ENV_FILE="${PROJECT_ROOT}/.env"
if [[ -f "${ENV_FILE}" ]]; then
  # shellcheck disable=SC1090  # dynamic path from repository root
  set -a
  source "${ENV_FILE}"
  set +a
  log "Loaded environment variables from ${ENV_FILE}"
fi

# Normalize Hugging Face token env var names used across tools/docs.
if [[ -z "${HF_TOKEN:-}" && -n "${HUGGING_FACE_HUB_TOKEN:-}" ]]; then
  export HF_TOKEN="${HUGGING_FACE_HUB_TOKEN}"
fi
if [[ -z "${HF_TOKEN:-}" && -n "${HF_HUB_TOKEN:-}" ]]; then
  export HF_TOKEN="${HF_HUB_TOKEN}"
fi

# Shared uv-managed environment (overridable via KOA_SHARED_ENV); lives outside job snapshots
SHARED_ENV_DIR="${KOA_SHARED_ENV:-${PROJECT_ROOT}/../.venv}"
mkdir -p "${SHARED_ENV_DIR}"
ENV_PYTHON="${SHARED_ENV_DIR}/bin/python"

SHARED_DIR="${PROJECT_ROOT}/../../.."

# Add uv to PATH
export PATH="${HOME}/.local/bin:${PATH}"

# Cache location for environment hashes (used to detect changes between runs)
ENV_CACHE_DIR="${SHARED_ENV_DIR}/.koa"
ENV_HASH_CACHE="${ENV_CACHE_DIR}/env_hashes.json"

# Locate the manifest generated by `koa submit` (contains env_hashes.json)
RUN_METADATA_DIR="${KOA_RUN_METADATA_DIR:-}"
if [[ -z "${RUN_METADATA_DIR}" && -n "${KOA_RUN_DIR:-}" ]]; then
  RUN_METADATA_DIR="${KOA_RUN_DIR}/run_metadata"
fi
if [[ -z "${RUN_METADATA_DIR}" && -n "${KOA_ML_RESULTS_ROOT:-}" && -n "${SLURM_JOB_ID:-}" ]]; then
  RUN_METADATA_DIR="${KOA_ML_RESULTS_ROOT}/${SLURM_JOB_ID}/run_metadata"
fi

log "Using container-provided CUDA toolchain; no custom setup required"

# Prefer python3, fall back to python; continue even if neither is available
python_bin=""
if command -v python3 >/dev/null 2>&1; then
  python_bin="$(command -v python3)"
elif command -v python >/dev/null 2>&1; then
  python_bin="$(command -v python)"
fi

if [[ -n "${python_bin}" ]]; then
  log "Found python interpreter at ${python_bin}"
else
  log "No system python found; relying on uv-managed interpreter"
fi

# Determine which extras to install (always include hpc; allow callers to add more)
UV_SYNC_EXTRAS="${UV_SYNC_EXTRAS:-}"
IFS=',' read -r -a _extras <<< "${UV_SYNC_EXTRAS}"
UV_SYNC_EXTRA_ARGS=()
for _extra in "${_extras[@]}"; do
  _trimmed="$(echo "${_extra}" | xargs)"
  if [[ -n "${_trimmed}" ]]; then
    UV_SYNC_EXTRA_ARGS+=(--extra "${_trimmed}")
  fi
done
if [[ ${#UV_SYNC_EXTRA_ARGS[@]} -gt 0 ]]; then
  log "uv sync extras: ${UV_SYNC_EXTRAS}"
fi

# Determine whether we need to rebuild or refresh the shared environment
recreate=0
if [[ ! -x "${ENV_PYTHON}" ]]; then
  recreate=1
fi

ENV_HASH_SOURCE=""
if [[ -n "${RUN_METADATA_DIR}" && -f "${RUN_METADATA_DIR}/env_hashes.json" ]]; then
  ENV_HASH_SOURCE="${RUN_METADATA_DIR}/env_hashes.json"
fi

# Compare the last synced env hashes with the current ones; rebuild if they differ
if [[ "${recreate}" -eq 0 && -n "${ENV_HASH_SOURCE}" ]]; then
  mkdir -p "${ENV_CACHE_DIR}"
  if [[ ! -f "${ENV_HASH_CACHE}" ]] || ! cmp -s "${ENV_HASH_SOURCE}" "${ENV_HASH_CACHE}"; then
    recreate=1
  fi
fi

export UV_PROJECT_ENVIRONMENT="${SHARED_ENV_DIR}"
if [[ "${recreate}" -eq 1 ]]; then
  log "Recreating shared environment at ${SHARED_ENV_DIR}"
  # Ensure uv is available for managing the shared environment
  if ! uv --help >/dev/null 2>&1; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
  fi

  # (Re)create the uv-managed environment and install dependencies from this repo snapshot
  uv venv --clear "${SHARED_ENV_DIR}"
  uv sync

  mkdir -p "${SHARED_DIR}/third_party"
  clone_or_fetch_isaaclab "${SHARED_DIR}/third_party/IsaacLab"
  cd "${REPO_DIR}"

  source "${SHARED_ENV_DIR}/bin/activate"
  export OMNI_KIT_ACCEPT_EULA=YES
  "${SHARED_DIR}/third_party/IsaacLab/isaaclab.sh" -i none

  uv pip install -e "${REPO_DIR}/source/lehome"

  # This creates the Assets/ directory with all required simulation resources
  if [[ -z "${HF_TOKEN:-}" ]]; then
    log "HF_TOKEN is not set; cannot download gated/rate-limited Hugging Face assets"
    return 1
  fi
  hf download lehome/asset_challenge --repo-type dataset --local-dir "${SHARED_DIR}/Assets"
  hf download lehome/dataset_challenge_merged --repo-type dataset --local-dir "${SHARED_DIR}/Datasets/example"


  if [[ -n "${ENV_HASH_SOURCE}" ]]; then
    mkdir -p "${ENV_CACHE_DIR}"
    cp "${ENV_HASH_SOURCE}" "${ENV_HASH_CACHE}"
  fi
else
  log "Shared environment already up to date; skipping rebuild"
fi
